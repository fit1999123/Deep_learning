import tensorflow as tf
import numpy as np
import os
from keras.models import Model
from keras.layers import Dense, Conv2D, Input, Flatten, BatchNormalization, Lambda, Reshape,ReLU,MaxPool2D,UpSampling2D
from keras.optimizers import Adam
from keras import backend as K
from create_label import x_train,x_test


cpu = False 


physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)


if cpu:

    os.environ["CUDA_VISIBLE_DEVICES"]='-1' 



img_size = 256
num_channels = 3

x = Input(shape=(img_size, img_size, num_channels), name="encoder_input")

encoder_conv_layer1 = Conv2D(filters=32, kernel_size=(3, 3), padding="same", strides=1, name="encoder_conv_1")(x)
encoder_norm_layer1 = BatchNormalization(name="encoder_norm_1")(encoder_conv_layer1)
encoder_activ_layer1 = ReLU(name="encoder_relu_1")(encoder_norm_layer1)
encoder_maxpool_layer1 = MaxPool2D(pool_size=(2, 2), strides=2, padding="same")(encoder_activ_layer1)



encoder_conv_layer2 = Conv2D(filters=64, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_2")(encoder_maxpool_layer1)
encoder_norm_layer2 = BatchNormalization(name="encoder_norm_2")(encoder_conv_layer2)
encoder_activ_layer2 = ReLU(name="encoder_activ_layer_2")(encoder_norm_layer2)
encoder_maxpool_layer2 = MaxPool2D(pool_size=(2, 2), strides=2, padding="same")(encoder_activ_layer2)


encoder_conv_layer3 = Conv2D(filters=128, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_3")(encoder_maxpool_layer2)
encoder_norm_layer3 = BatchNormalization(name="encoder_norm_3")(encoder_conv_layer3)
encoder_activ_layer3 = ReLU(name="encoder_activ_layer_3")(encoder_norm_layer3)
encoder_maxpool_layer3 = MaxPool2D(pool_size=(2, 2), strides=2, padding="same")(encoder_activ_layer3)



encoder_conv_layer4 = Conv2D(filters=128, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_4")(encoder_maxpool_layer3)
encoder_norm_layer4 = BatchNormalization(name="encoder_norm_4")(encoder_conv_layer4)
encoder_activ_layer4 = ReLU(name="encoder_activ_layer_4")(encoder_norm_layer4)

encoder_conv_layer5 = Conv2D(filters=64, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_5")(encoder_activ_layer4)
encoder_norm_layer5 = BatchNormalization(name="encoder_norm_5")(encoder_conv_layer5)
encoder_activ_layer5 = ReLU(name="encoder_activ_layer_5")(encoder_norm_layer5)
encoder_maxpool_layer5 = MaxPool2D(pool_size=(2, 2), strides=2, padding="same")(encoder_activ_layer5)


encoder_conv_layer6 = Conv2D(filters=32, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_6")(encoder_maxpool_layer5)
encoder_norm_layer6 = BatchNormalization(name="encoder_norm_6")(encoder_conv_layer6)
encoder_activ_layer6 = ReLU(name="encoder_activ_layer_6")(encoder_norm_layer6)

encoder_conv_layer7 = Conv2D(filters=16, kernel_size=(3,3), padding="same", strides=1, name="encoder_conv_7")(encoder_activ_layer6)
encoder_norm_layer7 = BatchNormalization(name="encoder_norm_7")(encoder_conv_layer7)
encoder_activ_layer7 = ReLU(name="encoder_activ_layer_7")(encoder_norm_layer7)



latent_space_dim = 1024

shape_before_flatten = (16,16,4)

encoder_flatten = Flatten()(encoder_activ_layer7)

encoder_mu = Dense(units=latent_space_dim, name="encoder_mu")(encoder_flatten)
encoder_log_variance = Dense(units=latent_space_dim, name="encoder_log_variance")(encoder_flatten)

def sampling(mu_log_variance):
    mu, log_variance = mu_log_variance
    epsilon = K.random_normal(shape=K.shape(mu), mean=0.0, stddev=1.0)
    random_sample = mu + K.exp(log_variance/2) * epsilon
    return random_sample


encoder_output = Lambda(sampling, name="encoder_output")([encoder_mu, encoder_log_variance])

encoder = Model(x, encoder_output, name="encoder_model")

decoder_input = Input(shape=(latent_space_dim), name="decoder_input")
decoder_dense_layer1 = Dense(units=np.prod(shape_before_flatten), name="decoder_dense_1")(decoder_input)
decoder_reshape = Reshape(target_shape=shape_before_flatten)(decoder_dense_layer1)
decoder_dense_layer2= Dense(units=np.prod((16,16,16)))(decoder_reshape)




decoder_conv_tran_layer1 = Conv2D(filters=32, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_1")(decoder_dense_layer2)
decoder_norm_layer1 = BatchNormalization(name="decoder_norm_1")(decoder_conv_tran_layer1)
decoder_activ_layer1 = ReLU(name="decoder_relu_1")(decoder_norm_layer1)
decoder_upsample_layer1 = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(decoder_activ_layer1)

decoder_conv_tran_layer2 = Conv2D(filters=64, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_2")(decoder_upsample_layer1)
decoder_norm_layer2 = BatchNormalization(name="decoder_norm_2")(decoder_conv_tran_layer2)
decoder_activ_layer2 = ReLU(name="decoder_relu_2")(decoder_norm_layer2)
decoder_upsample_layer2 = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(decoder_activ_layer2)

decoder_conv_tran_layer3 = Conv2D(filters=128, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_3")(decoder_upsample_layer2)
decoder_norm_layer3 = BatchNormalization(name="decoder_norm_3")(decoder_conv_tran_layer3)
decoder_activ_layer3 = ReLU(name="decoder_relu_3")(decoder_norm_layer3)



decoder_conv_tran_layer4 = Conv2D(filters=128, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_4")(decoder_activ_layer3)
decoder_norm_layer4 = BatchNormalization(name="decoder_norm_4")(decoder_conv_tran_layer4)
decoder_activ_layer4 = ReLU(name="decoder_relu_4")(decoder_norm_layer4)
decoder_upsample_layer4 = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(decoder_activ_layer4)

decoder_conv_tran_layer5 = Conv2D(filters=64, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_5")(decoder_upsample_layer4)
decoder_norm_layer5 = BatchNormalization(name="decoder_norm_5")(decoder_conv_tran_layer5)
decoder_activ_layer5 = ReLU(name="decoder_relu_5")(decoder_norm_layer5)
decoder_upsample_layer5 = UpSampling2D(size=(2, 2), data_format=None, interpolation='nearest')(decoder_activ_layer5)

decoder_conv_tran_layer6 = Conv2D(filters=32, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_6")(decoder_upsample_layer5)
decoder_norm_layer6 = BatchNormalization(name="decoder_norm_6")(decoder_conv_tran_layer6)
decoder_activ_layer6 = ReLU(name="decoder_relu_6")(decoder_norm_layer6)

decoder_output = Conv2D(filters=3, kernel_size=(3, 3), padding="same", strides=1, name="decoder_conv_tran_7")(decoder_activ_layer6)











decoder = Model(decoder_input, decoder_output, name="decoder_model")
vae_input = Input(shape=(img_size, img_size, num_channels), name="VAE_input")

vae_encoder_output = encoder(vae_input)
vae_decoder_output = decoder(vae_encoder_output)
vae = Model(vae_input, vae_decoder_output, name="VAE")

def loss_func(encoder_mu, encoder_log_variance):
    # def vae_reconstruction_loss(y_true, y_predict):

    #     return tf.reduce_mean(tf.square(y_true- y_predict))


    # def vae_kl_loss(encoder_mu, encoder_log_variance):
    #     kl_loss = -0.5 * tf.reduce_mean(1.0 + encoder_log_variance - K.square(encoder_mu) - K.exp(encoder_log_variance),axis = 1)
    #     return kl_loss

    def vae_reconstruction_loss(y_true, y_predict):

        return tf.reduce_mean(K.square(tf.norm((y_true - y_predict),ord=2,axis = 1)))


    def vae_kl_loss(encoder_mu, encoder_log_variance):
        kl_loss = 0.5 * tf.reduce_mean(1.0 + encoder_log_variance - K.square(encoder_mu) - K.exp(encoder_log_variance),axis = 1)
        return kl_loss



    def vae_loss(y_true, y_predict):
        reconstruction_loss = vae_reconstruction_loss(y_true, y_predict)
        kl_loss = vae_kl_loss(y_true, y_predict)

        loss = reconstruction_loss + kl_loss
        return loss

    return vae_loss

vae.compile(optimizer=Adam(lr = 0.0001), loss=loss_func(encoder_mu, encoder_log_variance))

x_train = x_train.astype("float32") / 255.0 
x_test = x_test.astype("float32") / 255.0

x_train = np.reshape(x_train, newshape=(x_train.shape[0], x_train.shape[1], x_train.shape[2], num_channels)) 
x_test = np.reshape(x_test, newshape=(x_test.shape[0], x_train.shape[1], x_train.shape[2], num_channels))

vae.fit(x_train, x_train, epochs=30, batch_size=5, shuffle=True, validation_data=(x_test, x_test))
encoder.save("VAE_encoder_op3.h5") 
decoder.save("VAE_decoder_op3.h5") 
vae.save("VAE_op3.h5")
